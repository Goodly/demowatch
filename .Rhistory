print(col_position)
col_position <- col_position + 1
}
}
col_position <- 1
View(onehotted_ans_mat[1:10, 1:10])
View(all_q_ans_tbl)
all_q_ans_tbl
colnames(onehotted_ans_mat) <- columns
View(onehotted_ans_mat[1:10, 1:10])
all_q_ans_tbl
library(readr)
dat <- read_csv("df-canonicalization/data/textthresher/dfcrowd1dh-2018-06-21T01.csv")
library(dplyr)
library(reshape2)
### CSV processing
dat <- read_csv("~/df-canonicalization/data/textthresher/dfcrowd1dh-2018-06-21T01.csv")
grouped_dat <- dat %>%
group_by(task_pybossa_id,
contributor_id,
topic_number,
question_number,
answer_number) %>%
select(task_pybossa_id,
contributor_id,
topic_number,
question_number,
answer_number) %>%
group_by(task_pybossa_id ,
contributor_id,
topic_number,
question_number) %>%
summarize(answer_list = list(unique(answer_number)))
grouped_dat$question_number <- sapply(grouped_dat$question_number,
function(x) {if (x < 10) {
return(paste0(0, x))
} else {
return(as.character(x))}})
grouped_dat <- grouped_dat %>% mutate(question_number = paste(topic_number,
question_number,
sep = "."))
grouped_dat <- grouped_dat[, c(1, 2, 4, 5)]
all_q <- sort(unique(grouped_dat$question_number))
tasks_and_contributors <- grouped_dat %>%
group_by(task_pybossa_id, contributor_id) %>%
summarize() %>%
na.omit()
col_names <- c("task", "contributor", all_q)
all_answers <- matrix(0, ncol = length(all_q) + 2,
nrow = nrow(tasks_and_contributors))
all_answers[, 1:2] <- as.matrix(tasks_and_contributors)
colnames(all_answers) <- col_names
answer_formatter <- function(tbl_row) {
### retrieve task and contributor
task <- tasks_and_contributors[[tbl_row, 1]]
contributor <- tasks_and_contributors[[tbl_row, 2]]
### access the answers
sub_table <- grouped_dat %>%
filter(task_pybossa_id == task & contributor_id == contributor)
sub_answers <- as.data.frame(t(sub_table[, 3:4]))
### transform answers tall to wide
columns <- unlist(sub_answers[1, ])
sub_answers <- as.data.frame(sub_answers[c(FALSE, TRUE), ])
colnames(sub_answers) <- columns
### fill in columns for missing questions
missing_q <- setdiff(all_q, columns)
missing_ans <- as.data.frame(matrix(0, ncol = length(missing_q)))
colnames(missing_ans) <- missing_q
### bind to answered questions and reorder
all_q_ans <- cbind(sub_answers, missing_ans)[, all_q]
return(all_q_ans)
}
all_q_ans_tbl <- answer_formatter(1)
for (i in 2:nrow(tasks_and_contributors)) {
### format the answers
ans_tbl <- answer_formatter(i)
### bind to the first row
all_q_ans_tbl <- rbind(all_q_ans_tbl, ans_tbl)
}
### labelling which answers are list answers
listed <- matrix(FALSE, ncol = ncol(all_q_ans_tbl), nrow = nrow(all_q_ans_tbl))
for (i in 1:nrow(listed)) {
for (j in 1:ncol(listed)) {
if (length(all_q_ans_tbl[[i, j]]) > 1) {
listed[i, j] <- TRUE}}}
### expanding list answers(
unique_answers <- lapply(all_q_ans_tbl, function(lst) {return(unique(unlist(lst)))})
### expanding list answers(
onehotter <- function(df) {
# all answers in the dataset by question
unique_answers <- lapply(df, function(lst) {return(unique(unlist(lst)))})
# empty container the size of the fully onehot encoded dataframe
onehotted_ans_mat <- matrix(0, ncol = length(unlist(unique_answers)), nrow = nrow(df))
# produces the names of all the columns
columns <- list()
col_position <- 1
for (question in names(unique_answers)) {
answers <- unique_answers[[question]]
for (i in 1:length(answers)) {
if (answers[i] < 10) {
answer_number <- paste0("0", answers[i])
} else {
answer_number <- answers[i]
}
columns <- append(columns, paste0(question, ".", answer_number))
ans_in_col <- unlist(lapply(df[, c(question)], function(x) {answers[i] %in% x}))
onehotted_ans_mat[, col_position] <- ans_in_col
print(col_position)
col_position <- col_position + 1
}
}
colnames(onehotted_ans_mat) <- columns
}
onehot_answers <- onehotter(all_q_ans_tbl)
View(onehot_answers)
View(onehot_answers)
onehot_answers[1]
### expanding list answers(
onehotter <- function(df) {
# all answers in the dataset by question
unique_answers <- lapply(df, function(lst) {return(unique(unlist(lst)))})
# empty container the size of the fully onehot encoded dataframe
onehotted_ans_mat <- matrix(0, ncol = length(unlist(unique_answers)), nrow = nrow(df))
# produces the names of all the columns
columns <- list()
col_position <- 1
for (question in names(unique_answers)) {
answers <- unique_answers[[question]]
for (i in 1:length(answers)) {
if (answers[i] < 10) {
answer_number <- paste0("0", answers[i])
} else {
answer_number <- answers[i]
}
columns <- append(columns, paste0(question, ".", answer_number))
ans_in_col <- unlist(lapply(df[, c(question)], function(x) {answers[i] %in% x}))
onehotted_ans_mat[, col_position] <- ans_in_col
print(col_position)
col_position <- col_position + 1
}
}
colnames(onehotted_ans_mat) <- columns
return(onehotted_ans_matrix)
}
onehot_answers <- onehotter(all_q_ans_tbl)
return(onehotted_ans_mat)
### expanding list answers(
onehotter <- function(df) {
# all answers in the dataset by question
unique_answers <- lapply(df, function(lst) {return(unique(unlist(lst)))})
# empty container the size of the fully onehot encoded dataframe
onehotted_ans_mat <- matrix(0, ncol = length(unlist(unique_answers)), nrow = nrow(df))
# produces the names of all the columns
columns <- list()
col_position <- 1
for (question in names(unique_answers)) {
answers <- unique_answers[[question]]
for (i in 1:length(answers)) {
if (answers[i] < 10) {
answer_number <- paste0("0", answers[i])
} else {
answer_number <- answers[i]
}
columns <- append(columns, paste0(question, ".", answer_number))
ans_in_col <- unlist(lapply(df[, c(question)], function(x) {answers[i] %in% x}))
onehotted_ans_mat[, col_position] <- ans_in_col
print(col_position)
col_position <- col_position + 1
}
}
colnames(onehotted_ans_mat) <- columns
return(onehotted_ans_mat)
}
onehot_answers <- onehotter(all_q_ans_tbl)
View(onehot_answers)
### expanding list answers(
onehotter <- function(df) {
# all answers in the dataset by question
unique_answers <- lapply(df, function(lst) {return(unique(unlist(lst)))})
# empty container the size of the fully onehot encoded dataframe
onehotted_ans_mat <- matrix(0, ncol = length(unlist(unique_answers)), nrow = nrow(df))
# produces the names of all the columns
columns <- list()
col_position <- 1
for (question in names(unique_answers)) {
answers <- unique_answers[[question]]
for (i in 1:length(answers)) {
if (answers[i] < 10) {
answer_number <- paste0("0", answers[i])
} else {
answer_number <- answers[i]
}
columns <- append(columns, paste0(question, ".", answer_number))
ans_in_col <- unlist(lapply(df[, c(question)], function(x) {answers[i] %in% x}))
onehotted_ans_mat[, col_position] <- ans_in_col
print(col_position)
col_position <- col_position + 1
}
}
colnames(onehotted_ans_mat) <- columns
return(as.data.frame(onehotted_ans_mat))
}
onehot_answers <- onehotter(all_q_ans_tbl)
library(readr)
dat <- read_csv("df-canonicalization/data/textthresher/dfcrowd1dh-2018-06-21T01.csv")
library(readr)
library(dplyr)
library(reshape2)
library(cluster)
library(ggplot2)
library(fpc)
grouped_dat <- dat %>%
group_by(task_pybossa_id,
contributor_id,
topic_number,
question_number,
answer_number) %>%
select(task_pybossa_id,
contributor_id,
topic_number,
question_number,
answer_number) %>%
group_by(task_pybossa_id ,
contributor_id,
topic_number,
question_number) %>%
summarize(answer_list = list(unique(answer_number)))
grouped_dat$question_number <- sapply(grouped_dat$question_number,
function(x) {if (x < 10) {
return(paste0(0, x))
} else {
return(as.character(x))}})
grouped_dat <- grouped_dat %>% mutate(question_number = paste(topic_number,
question_number,
sep = "."))
grouped_dat <- grouped_dat[, c(1, 2, 4, 5)]
all_q <- sort(unique(grouped_dat$question_number))
tasks_and_contributors <- grouped_dat %>%
group_by(task_pybossa_id, contributor_id) %>%
summarize() %>%
na.omit()
col_names <- c("task", "contributor", all_q)
all_answers <- matrix(0, ncol = length(all_q) + 2,
nrow = nrow(tasks_and_contributors))
all_answers[, 1:2] <- as.matrix(tasks_and_contributors)
colnames(all_answers) <- col_names
answer_formatter <- function(tbl_row) {
### retrieve task and contributor
task <- tasks_and_contributors[[tbl_row, 1]]
contributor <- tasks_and_contributors[[tbl_row, 2]]
### access the answers
sub_table <- grouped_dat %>%
filter(task_pybossa_id == task & contributor_id == contributor)
sub_answers <- as.data.frame(t(sub_table[, 3:4]))
### transform answers tall to wide
columns <- unlist(sub_answers[1, ])
sub_answers <- as.data.frame(sub_answers[c(FALSE, TRUE), ])
colnames(sub_answers) <- columns
### fill in columns for missing questions
missing_q <- setdiff(all_q, columns)
missing_ans <- as.data.frame(matrix(0, ncol = length(missing_q)))
colnames(missing_ans) <- missing_q
### bind to answered questions and reorder
all_q_ans <- cbind(sub_answers, missing_ans)[, all_q]
return(all_q_ans)
}
all_q_ans_tbl <- answer_formatter(1)
for (i in 2:nrow(tasks_and_contributors)) {
### format the answers
ans_tbl <- answer_formatter(i)
### bind to the first row
all_q_ans_tbl <- rbind(all_q_ans_tbl, ans_tbl)
}
### expanding list answers(
onehotter <- function(df) {
# all answers in the dataset by question
unique_answers <- lapply(df, function(lst) {return(unique(unlist(lst)))})
# empty container the size of the fully onehot encoded dataframe
onehotted_ans_mat <- matrix(0, ncol = length(unlist(unique_answers)), nrow = nrow(df))
# produces the names of all the columns
columns <- list()
col_position <- 1
for (question in names(unique_answers)) {
answers <- unique_answers[[question]]
for (i in 1:length(answers)) {
if (answers[i] < 10) {
answer_number <- paste0("0", answers[i])
} else {
answer_number <- answers[i]
}
columns <- append(columns, paste0(question, ".", answer_number))
ans_in_col <- unlist(lapply(df[, c(question)], function(x) {answers[i] %in% x}))
onehotted_ans_mat[, col_position] <- ans_in_col
print(col_position)
col_position <- col_position + 1
}
}
colnames(onehotted_ans_mat) <- columns
return(as.data.frame(onehotted_ans_mat))
}
onehot_answers <- onehotter(all_q_ans_tbl)
final_answers_w_metadata <- cbind(as.data.frame(tasks_and_contributors),
onehot_answers)
IDs <- readRDS('data/tuas_with_ids.rds')
### joining IDs into features
ids_and_features <- final_answers_w_metadata %>%
inner_join(IDs, by = c("task_pybossa_id" = "task_id")) %>%
select(-c(article_number,
case_number,
event_place,
date_published,
TUA,
article_text,
filename,
event_date))
### calculate dissimilarity matrix
gower_dist <- ids_and_features %>%
filter(ids == 1) %>%
select(-c(task_pybossa_id, contributor_id, ids)) %>%
daisy(metric = c("gower"))
### clustering
aggl_clust <- hclust(gower_dist, method = "complete")
### cluster analysis
sil_widths <- c()
for (i in 2:6) {
sil_widths[i - 1] <- cluster.stats(gower_dist, clustering = cutree(aggl_clust, k = i))["avg.silwidth"][[1]]
}
setwd('Desktop/df-canonicalization')
setwd('/Desktop/df-canonicalization')
setwd('C:/Users/Sidney//Desktop/df-canonicalization')
IDs <- readRDS('data/tuas_with_ids.rds')
### joining IDs into features
ids_and_features <- final_answers_w_metadata %>%
inner_join(IDs, by = c("task_pybossa_id" = "task_id")) %>%
select(-c(article_number,
case_number,
event_place,
date_published,
TUA,
article_text,
filename,
event_date))
### calculate dissimilarity matrix
gower_dist <- ids_and_features %>%
filter(ids == 1) %>%
select(-c(task_pybossa_id, contributor_id, ids)) %>%
daisy(metric = c("gower"))
### clustering
aggl_clust <- hclust(gower_dist, method = "complete")
### cluster analysis
sil_widths <- c()
for (i in 2:6) {
sil_widths[i - 1] <- cluster.stats(gower_dist, clustering = cutree(aggl_clust, k = i))["avg.silwidth"][[1]]
}
aggl_clust
cutree(aggl_clust, k = 1
)
cutree(aggl_clust, k = 1)
cluster.stats(gower_dist, clustering = cutree(aggl_clust, k = 1))
cluster.stats(gower_dist, clustering = cutree(aggl_clust, k = 2))
wss <- function(feat_tbl) {
centroid <- apply(feat_tbl, 2, fun)
dists <- apply(feat_tbl, 1, function(x) {return(dist_sq(x, centroid))})
return(dists)
}
test_features <- ids_and_features %>% filter(ids == 1) %>% select(-c(task_pybossa_id, contributor_id, ids_and_features))
test_features <- ids_and_features %>% filter(ids == 1) %>% select(-c(task_pybossa_id, contributor_id, ids))
wss(test_features)
return(sum((row_a - row_b)^2))
### cluster analysis
dist_sq <- function(row_a, row_b) {
return(sum((row_a - row_b)^2))
}
wss <- function(feat_tbl) {
centroid <- apply(feat_tbl, 2, fun)
dists <- apply(feat_tbl, 1, function(x) {return(dist_sq(x, centroid))})
return(dists)
}
wss <- function(feat_tbl) {
centroid <- apply(feat_tbl, 2, mean)
dists <- apply(feat_tbl, 1, function(x) {return(dist_sq(x, centroid))})
return(dists)
}
wss(test_features)
test_features
centroid <- apply(test_features, 2, mean)
centroid
wss(test_features)
return(sum(dists))
### cluster analysis
dist_sq <- function(row_a, row_b) {return(sum((row_a - row_b)^2))}
wss <- function(feat_tbl) {
centroid <- apply(feat_tbl, 2, mean)
dists <- apply(feat_tbl, 1, function(x) {return(dist_sq(x, centroid))})
return(sum(dists))
}
?cluster.stats
gowers_dist
gower_dist
test_features[1, ]
test_features[1, ] == 1
((test_features[1, ] == 1) == (test_features[2, ] == 1))
gower_dist^2
gower_dist
gower_dist^2
sum(gower_dist^2)
sum(gower_dist^2)/(2*7)
wcss <- sum(gower_dist^2)/(2*7)
cluster.stats(gower_dist, clustering = cutree(aggl_clust, k = 2))
### clustering
weights <- read_csv('data/scheme_q_types.csv')
View(weights)
?hclust
weights[, 2][weights[, 2] == "Additive"] <- 0
weights[, 2][weights[, 2] == "Ambiguous"] <- 0.5
weights[, 2][weights[, 2] == "Contradictory"] <- 1
weights[, 2] <- as.numeric(weights[, 2])
weights[, 2][weights[, 2] == "Additive"] <- 0.1
weights[, 2][weights[, 2] == "Ambiguous"] <- 0.5
weights[, 2][weights[, 2] == "Contradictory"] <- 1
weights[, 2] <- as.numeric(weights[, 2])
mutate?
?mutate
weights <- weights %>% mutate(weights = as.numeric(Type))
weights <- weights %>% mutate(weights = as.numeric(Type)) %>% select(1, 4)
?daisy
gower_dist <- ids_and_features %>%
filter(ids == 1) %>%
select(-c(task_pybossa_id, contributor_id, ids)) %>%
daisy(metric = c("gower"), weights = weights[, 2])
weights
weights[, 2]
weights[[, 2]]
weights$weights
gower_dist <- ids_and_features %>%
filter(ids == 1) %>%
select(-c(task_pybossa_id, contributor_id, ids)) %>%
daisy(metric = c("gower"), weights = weights$weights)
length(weights$weights)
ncol(test_features)
all_q
q_ans_combos <- colnames(ids_and_features)
q_ans_combos <- data.frame(col = colnames(ids_and_features))
View(q_ans_combos)
q_ans_combos <- data.frame(col = colnames(ids_and_features))[-c(1, 2, 297), ]
q_ans_combos <- data.frame(col = colnames(ids_and_features[, -c(1, 2, 297)]))
q_ans_combos$q_name <- substr(q_ans_combos$col, 0, 3)
q_ans_combos$q_name <- substr(q_ans_combos$col, 0, 4)
weights <- weights %>% inner_join(q_ans_combos, by = c("Number" = "q_name"))
weights <- weights %>% mutate(weights = as.numeric(Type), q_name = as.character(Number)) %>% select(5, 4)
### calculate weighted dissimilarity matrix
weights <- read_csv('data/scheme_q_types.csv')
weights[, 2][weights[, 2] == "Additive"] <- 0.1
weights[, 2][weights[, 2] == "Ambiguous"] <- 0.5
weights[, 2][weights[, 2] == "Contradictory"] <- 1
weights <- weights %>% mutate(weights = as.numeric(Type), q_name = as.character(Number)) %>% select(5, 4)
weights <- weights %>% inner_join(q_ans_combos, by = c("q_name"))
setdiff(weights$col, q_ans_combos$col)
weights <- weights %>% right_join(q_ans_combos, by = c("q_name"))
### calculate weighted dissimilarity matrix
weights <- read_csv('data/scheme_q_types.csv')
weights[, 2][weights[, 2] == "Additive"] <- 0.1
weights[, 2][weights[, 2] == "Ambiguous"] <- 0.5
weights[, 2][weights[, 2] == "Contradictory"] <- 1
weights <- weights %>% mutate(weights = as.numeric(Type), q_name = as.character(Number)) %>% select(5, 4)
q_ans_combos <- data.frame(col = colnames(ids_and_features[, -c(1, 2, 297)]))
q_ans_combos$q_name <- substr(q_ans_combos$col, 0, 4)
weights <- weights %>% right_join(q_ans_combos, by = c("q_name"))
weights$weights[is.na(weights$weights)] <- 0
gower_dist <- ids_and_features %>%
filter(ids == 1) %>%
select(-c(task_pybossa_id, contributor_id, ids)) %>%
daisy(metric = c("gower"), weights = weights$weights)
gower_dist <- ids_and_features %>%
filter(ids == 1) %>%
select(-c(task_pybossa_id, contributor_id, ids))
gower_dist <- ids_and_features %>%
filter(ids == 1) %>%
select(-c(task_pybossa_id, contributor_id, ids)) %>%
daisy(metric = c("gower"), weights = weights$weights)
gower_dist
### clustering
aggl_clust <- hclust(gower_dist, method = "complete")
?cluster.stats
sil_widths <- cluster.stats(gower_dist)$clus.avg.silwidths
sil_widths <- cluster.stats(gower_dist, clustering = aggl_clust)$clus.avg.silwidths
sil_widths <- cluster.stats(gower_dist, clustering = cutree(aggl_clust, k = 2))$clus.avg.silwidths
sil_widths
sil_widths <- cluster.stats(gower_dist, clustering = cutree(aggl_clust, k = 6))$clus.avg.silwidths
sil_widths
sil_widths <- cluster.stats(gower_dist, clustering = cutree(aggl_clust))$clus.avg.silwidths
sil_widths <- c()
for (i in 2:6) {
sil_widths[i - 1] <- cluster.stats(gower_dist, clustering = cutree(aggl_clust, k = i))["avg.silwidth"][[1]]
}
sum(gower_dist^2)/(2*7)
wcss <- c()
for (i in 2:6) {
wcss[i - 1] <- cluster.stats(gower_dist, clustering = cutree(aggl_clust, k = i))["within.cluster.ss"][[1]]
}
wcss
plot(aggl_clust)
### clustering
aggl_clust <- hclust(gower_dist, method = "average")
plot(aggl_clust)
### clustering
aggl_clust <- hclust(gower_dist, method = "single")
plot(aggl_clust)
### clustering
aggl_clust <- hclust(gower_dist, method = "complete")
cluster.stats(gower_dist, clustering = cutree(aggl_clust, k = 2))
plot(aggl_clust)
