missing_ans <- as.data.frame(matrix(0, ncol = length(missing_q)))
colnames(missing_ans) <- missing_q
### bind to answered questions and reorder
all_q_ans <- cbind(sub_answers, missing_ans)[, all_q]
return(all_q_ans)
}
all_q_ans_tbl <- answer_formatter(1)
for (i in 2:nrow(tasks_and_contributors)) {
### format the answers
ans_tbl <- answer_formatter(i)
### bind to the first row
all_q_ans_tbl <- rbind(all_q_ans_tbl, ans_tbl)
}
### labelling which answers are list answers
listed <- matrix(FALSE, ncol = ncol(all_q_ans_tbl), nrow = nrow(all_q_ans_tbl))
for (i in 1:nrow(listed)) {
for (j in 1:ncol(listed)) {
if (length(all_q_ans_tbl[[i, j]]) > 1) {
listed[i, j] <- TRUE}}}
### expanding list answers(
unique_answers <- lapply(all_q_ans_tbl, function(lst) {return(unique(unlist(lst)))})
### expanding list answers(
onehotter <- function(df) {
# all answers in the dataset by question
unique_answers <- lapply(df, function(lst) {return(unique(unlist(lst)))})
# empty container the size of the fully onehot encoded dataframe
onehotted_ans_mat <- matrix(0, ncol = length(unlist(unique_answers)), nrow = nrow(df))
# produces the names of all the columns
columns <- list()
col_position <- 1
for (question in names(unique_answers)) {
answers <- unique_answers[[question]]
for (i in 1:length(answers)) {
if (answers[i] < 10) {
answer_number <- paste0("0", answers[i])
} else {
answer_number <- answers[i]
}
columns <- append(columns, paste0(question, ".", answer_number))
ans_in_col <- unlist(lapply(df[, c(question)], function(x) {answers[i] %in% x}))
onehotted_ans_mat[, col_position] <- ans_in_col
print(col_position)
col_position <- col_position + 1
}
}
colnames(onehotted_ans_mat) <- columns
}
onehot_answers <- onehotter(all_q_ans_tbl)
View(onehot_answers)
View(onehot_answers)
onehot_answers[1]
### expanding list answers(
onehotter <- function(df) {
# all answers in the dataset by question
unique_answers <- lapply(df, function(lst) {return(unique(unlist(lst)))})
# empty container the size of the fully onehot encoded dataframe
onehotted_ans_mat <- matrix(0, ncol = length(unlist(unique_answers)), nrow = nrow(df))
# produces the names of all the columns
columns <- list()
col_position <- 1
for (question in names(unique_answers)) {
answers <- unique_answers[[question]]
for (i in 1:length(answers)) {
if (answers[i] < 10) {
answer_number <- paste0("0", answers[i])
} else {
answer_number <- answers[i]
}
columns <- append(columns, paste0(question, ".", answer_number))
ans_in_col <- unlist(lapply(df[, c(question)], function(x) {answers[i] %in% x}))
onehotted_ans_mat[, col_position] <- ans_in_col
print(col_position)
col_position <- col_position + 1
}
}
colnames(onehotted_ans_mat) <- columns
return(onehotted_ans_matrix)
}
onehot_answers <- onehotter(all_q_ans_tbl)
return(onehotted_ans_mat)
### expanding list answers(
onehotter <- function(df) {
# all answers in the dataset by question
unique_answers <- lapply(df, function(lst) {return(unique(unlist(lst)))})
# empty container the size of the fully onehot encoded dataframe
onehotted_ans_mat <- matrix(0, ncol = length(unlist(unique_answers)), nrow = nrow(df))
# produces the names of all the columns
columns <- list()
col_position <- 1
for (question in names(unique_answers)) {
answers <- unique_answers[[question]]
for (i in 1:length(answers)) {
if (answers[i] < 10) {
answer_number <- paste0("0", answers[i])
} else {
answer_number <- answers[i]
}
columns <- append(columns, paste0(question, ".", answer_number))
ans_in_col <- unlist(lapply(df[, c(question)], function(x) {answers[i] %in% x}))
onehotted_ans_mat[, col_position] <- ans_in_col
print(col_position)
col_position <- col_position + 1
}
}
colnames(onehotted_ans_mat) <- columns
return(onehotted_ans_mat)
}
onehot_answers <- onehotter(all_q_ans_tbl)
View(onehot_answers)
### expanding list answers(
onehotter <- function(df) {
# all answers in the dataset by question
unique_answers <- lapply(df, function(lst) {return(unique(unlist(lst)))})
# empty container the size of the fully onehot encoded dataframe
onehotted_ans_mat <- matrix(0, ncol = length(unlist(unique_answers)), nrow = nrow(df))
# produces the names of all the columns
columns <- list()
col_position <- 1
for (question in names(unique_answers)) {
answers <- unique_answers[[question]]
for (i in 1:length(answers)) {
if (answers[i] < 10) {
answer_number <- paste0("0", answers[i])
} else {
answer_number <- answers[i]
}
columns <- append(columns, paste0(question, ".", answer_number))
ans_in_col <- unlist(lapply(df[, c(question)], function(x) {answers[i] %in% x}))
onehotted_ans_mat[, col_position] <- ans_in_col
print(col_position)
col_position <- col_position + 1
}
}
colnames(onehotted_ans_mat) <- columns
return(as.data.frame(onehotted_ans_mat))
}
onehot_answers <- onehotter(all_q_ans_tbl)
library(readr)
dat <- read_csv("df-canonicalization/data/textthresher/dfcrowd1dh-2018-06-21T01.csv")
library(readr)
library(dplyr)
library(cluster)
library(ggplot2)
library(fpc)
grouped_dat <- dat %>%
group_by(task_pybossa_id,
contributor_id,
topic_number,
question_number,
answer_number) %>%
select(task_pybossa_id,
contributor_id,
topic_number,
question_number,
answer_number) %>%
group_by(task_pybossa_id ,
contributor_id,
topic_number,
question_number) %>%
summarize(answer_list = list(unique(answer_number)))
grouped_dat$question_number <- sapply(grouped_dat$question_number,
function(x) {if (x < 10) {
return(paste0(0, x))
} else {
return(as.character(x))}})
grouped_dat <- grouped_dat %>% mutate(question_number = paste(topic_number,
question_number,
sep = "."))
grouped_dat <- grouped_dat[, c(1, 2, 4, 5)]
all_q <- sort(unique(grouped_dat$question_number))
tasks_and_contributors <- grouped_dat %>%
group_by(task_pybossa_id, contributor_id) %>%
summarize() %>%
na.omit()
col_names <- c("task", "contributor", all_q)
setwd('df-canonicalization')
all_answers <- matrix(0, ncol = length(all_q) + 2,
nrow = nrow(tasks_and_contributors))
all_answers[, 1:2] <- as.matrix(tasks_and_contributors)
colnames(all_answers) <- col_names
answer_formatter <- function(tbl_row) {
### retrieve task and contributor
task <- tasks_and_contributors[[tbl_row, 1]]
contributor <- tasks_and_contributors[[tbl_row, 2]]
### access the answers
sub_table <- grouped_dat %>%
filter(task_pybossa_id == task & contributor_id == contributor)
sub_answers <- as.data.frame(t(sub_table[, 3:4]))
### transform answers tall to wide
columns <- unlist(sub_answers[1, ])
sub_answers <- as.data.frame(sub_answers[c(FALSE, TRUE), ])
colnames(sub_answers) <- columns
### fill in columns for missing questions
missing_q <- setdiff(all_q, columns)
missing_ans <- as.data.frame(matrix(0, ncol = length(missing_q)))
colnames(missing_ans) <- missing_q
### bind to answered questions and reorder
all_q_ans <- cbind(sub_answers, missing_ans)[, all_q]
return(all_q_ans)
}
all_q_ans_tbl <- answer_formatter(1)
for (i in 2:nrow(tasks_and_contributors)) {
### format the answers
ans_tbl <- answer_formatter(i)
### bind to the first row
all_q_ans_tbl <- rbind(all_q_ans_tbl, ans_tbl)
}
### expanding list answers(
onehotter <- function(df) {
# all answers in the dataset by question
unique_answers <- lapply(df, function(lst) {return(unique(unlist(lst)))})
# empty container the size of the fully onehot encoded dataframe
onehotted_ans_mat <- matrix(0, ncol = length(unlist(unique_answers)), nrow = nrow(df))
# produces the names of all the columns
columns <- list()
col_position <- 1
for (question in names(unique_answers)) {
answers <- unique_answers[[question]]
for (i in 1:length(answers)) {
if (answers[i] < 10) {
answer_number <- paste0("0", answers[i])
} else {
answer_number <- answers[i]
}
columns <- append(columns, paste0(question, ".", answer_number))
ans_in_col <- unlist(lapply(df[, c(question)], function(x) {answers[i] %in% x}))
onehotted_ans_mat[, col_position] <- ans_in_col
print(col_position)
col_position <- col_position + 1
}
}
colnames(onehotted_ans_mat) <- columns
return(as.data.frame(onehotted_ans_mat))
}
onehot_answers <- onehotter(all_q_ans_tbl)
final_answers_w_metadata <- cbind(as.data.frame(tasks_and_contributors),
onehot_answers)
IDs <- readRDS('data/tuas_with_ids.rds')
### joining IDs into features
ids_and_features <- final_answers_w_metadata %>%
inner_join(IDs, by = c("task_pybossa_id" = "task_id")) %>%
select(-c(article_number,
case_number,
event_place,
date_published,
TUA,
article_text,
filename,
event_date))
### calculate question weights
weights <- read_csv('data/scheme_q_types.csv')
weights[, 2][weights[, 2] == "Additive"] <- 0.1
weights[, 2][weights[, 2] == "Ambiguous"] <- 0.5
weights[, 2][weights[, 2] == "Contradictory"] <- 1
weights <- weights %>% mutate(weights = as.numeric(Type), q_name = as.character(Number)) %>% select(5, 4)
q_ans_combos <- data.frame(col = colnames(ids_and_features[, -c(1, 2, 297)]))
q_ans_combos$q_name <- substr(q_ans_combos$col, 0, 4)
weights <- weights %>% right_join(q_ans_combos, by = c("q_name"))
weights$weights[is.na(weights$weights)] <- 0
write_csv(weights, "data/scheme_q_weights.csv")
weights <- read.csv('data/scheme_q_weights.csv')
check_cluster <- function(tbl, id, pg_cutoff = 0.9) {
require(dplyr)
require(cluster)
require(fpc)
# weights will be defined outside and are constant regardless of what rows
# are input
# dissimilarity
gower_dist <- tbl %>%
filter(ids == id) %>%
select(-c(task_pybossa_id, contributor_id, ids)) %>%
daisy(metric = c("gower"), weights = weights$weights)
# clustering
aggl_clust <- hclust(gower_dist, method = "complete")
# analysis
pg <- c()
for (i in 2:6) {
pg[i - 1] <-
cluster.stats(gower_dist,
clustering = cutree(aggl_clust, k = i))["pearsongamma"][[1]]
if (pg[i - 1] > pg_cutoff) {
return(tbl %>%
filter(ids == id) %>%
select(c(task_pybossa_id, contributor_id)) %>%
mutate(cluster = cutree(aggl_clust, k = i)))
}
}
return(tbl %>%
filter(ids == id) %>%
select(c(task_pybossa_id, contributor_id)) %>%
mutate(cluster = 1))
}
check_cluster <- function(tbl, id, pg_cutoff = 0.9) {
require(dplyr)
require(cluster)
require(fpc)
# weights will be defined outside and are constant regardless of what rows
# are input
# dissimilarity
gower_dist <- tbl %>%
filter(ids == id) %>%
select(-c(task_pybossa_id, contributor_id, ids)) %>%
daisy(metric = c("gower"), weights = weights$weights)
# clustering
aggl_clust <- hclust(gower_dist, method = "complete")
plot(aggl_clust)
# analysis
pg <- c()
for (i in 2:6) {
pg[i - 1] <-
cluster.stats(gower_dist,
clustering = cutree(aggl_clust, k = i))["pearsongamma"][[1]]
if (pg[i - 1] > pg_cutoff) {
return(tbl %>%
filter(ids == id) %>%
select(c(task_pybossa_id, contributor_id)) %>%
mutate(cluster = cutree(aggl_clust, k = i)))
}
}
return(tbl %>%
filter(ids == id) %>%
select(c(task_pybossa_id, contributor_id)) %>%
mutate(cluster = 1))
}
View(weights)
check_cluster(ids_and_features, 1)
check_cluster <- function(tbl, id, pg_cutoff = 0.9) {
require(dplyr)
require(cluster)
require(fpc)
# weights will be defined outside and are constant regardless of what rows
# are input
# dissimilarity
gower_dist <- tbl %>%
filter(ids == id) %>%
select(-c(task_pybossa_id, contributor_id, ids)) %>%
daisy(metric = c("gower"), weights = weights$weights)
# clustering
aggl_clust <- hclust(gower_dist, method = "complete")
plot(aggl_clust)
# analysis
pg <- c()
for (i in 2:6) {
pg[i - 1] <-
cluster.stats(gower_dist,
clustering = cutree(aggl_clust, k = i))["pearsongamma"][[1]]
if (pg[i - 1] > pg_cutoff) {
return(tbl %>%
filter(ids == id) %>%
select(c(task_pybossa_id, contributor_id)) %>%
mutate(cluster = cutree(aggl_clust, k = i)))
}
}
return(tbl %>%
filter(ids == id) %>%
select(c(task_pybossa_id, contributor_id)) %>%
mutate(cluster = 1))
}
check_cluster(ids_and_features, 1)
check_cluster(ids_and_features, 1, pg_cutoff = 0.5)
check_cluster(ids_and_features, 2, pg_cutoff = 0.5)
check_cluster(ids_and_features, 3, pg_cutoff = 0.5)
check_cluster(ids_and_features, 4, pg_cutoff = 0.5)
check_cluster(ids_and_features, 5, pg_cutoff = 0.5)
ids_and_features %>% filter(ids == 5)
check_cluster(ids_and_features, 5, pg_cutoff = 0.9)
ids_and_features %>% filter(ids == 5) %>% select(1)
ids_and_features %>% filter(ids == 5) %>% select(3)
### calculate question weights
weights <- read_csv('data/scheme_q_types.csv')
View(weights)
weights[, 2][weights[, 2] == "Additive"] <- 0.1
weights[, 2][weights[, 2] == "Ambiguous"] <- 0.5
weights[, 2][weights[, 2] == "Contradictory"] <- 1
weights <- weights %>% mutate(weights = as.numeric(Type), q_name = as.character(Number)) %>% select(5, 4)
### calculate question weights
weights <- read_csv('data/scheme_q_types.csv')
weights[, 2][weights[, 2] == "Additive"] <- 0.1
weights[, 2][weights[, 2] == "Ambiguous"] <- 0.5
weights[, 2][weights[, 2] == "Contradictory"] <- 1
6.10
as.character(6.10)
as.character(6.10) == as.character(6.1)
6.10
6.10
for (i in 1:length(weights$Number)) {
if (length(weights$Number[[i]]) != 4) {
weights$Number[[i]] <- paste0(weights$Number[[i]], "0")
}
}
### calculate question weights
weights <- read_csv('data/scheme_q_types.csv')
weights[, 2][weights[, 2] == "Additive"] <- 0.1
weights[, 2][weights[, 2] == "Ambiguous"] <- 0.5
weights[, 2][weights[, 2] == "Contradictory"] <- 1
weights <- weights %>% mutate(weights = as.numeric(Type), q_name = as.character(Number)) %>% select(5, 4)
nchar("0.01")
for (i in 1:length(weights$Number)) {
if (nchar(weights$Number[[i]]) != 4) {
weights$Number[[i]] <- paste0(weights$Number[[i]], "0")
}
}
for (i in 1:length(weights$Number)) {
if (nchar(weights$Number[i]) != 4) {
weights$Number[i] <- paste0(weights$Number[[i]], "0")
}
}
4 != 5
View(ids_and_features)
check_cluster(ids_and_features, 1, pg_cutoff = 0.9)
weights <- read.csv('data/scheme_q_weights.csv')
check_cluster(ids_and_features, 1, pg_cutoff = 0.9)
check_cluster(ids_and_features, 1, pg_cutoff = 0.8)
check_cluster(ids_and_features, 1, pg_cutoff = 0.9)
check_cluster(ids_and_features, 4, pg_cutoff = 0.9)
check_cluster(ids_and_features, 6, pg_cutoff = 0.9)
check_cluster(ids_and_features, 5, pg_cutoff = 0.9)
check_cluster(ids_and_features, 7, pg_cutoff = 0.9)
### calculate question weights
weights <- read_csv('data/scheme_q_types.csv')
weights[, 2][weights[, 2] == "Additive"] <- 0.1
weights[, 2][weights[, 2] == "Ambiguous"] <- 0.5
weights[, 2][weights[, 2] == "Contradictory"] <- 1
weights <- weights %>% mutate(weights = as.numeric(Type), q_name = as.character(Number)) %>% select(5, 4)
for (i in 1:length(weights$Number)) {
if (nchar(weights$Number[i]) != 4) {
weights$Number[i] <- paste0(weights$Number[[i]], "0")
}
}
### calculate question weights
weights <- read_csv('data/scheme_q_types.csv')
weights[, 2][weights[, 2] == "Additive"] <- 0.1
weights[, 2][weights[, 2] == "Ambiguous"] <- 0.5
weights[, 2][weights[, 2] == "Contradictory"] <- 1
weights <- weights %>% mutate(weights = as.numeric(Type), q_name = as.character(Number)) %>% select(5, 4)
for (i in 1:length(weights$Number)) {
# if (nchar(weights$Number[i]) != 4) {
#   weights$Number[i] <- paste0(weights$Number[[i]], "0")
# }
print(i)
}
for (i in 1:length(weights$q_name)) {
# if (nchar(weights$Number[i]) != 4) {
#   weights$Number[i] <- paste0(weights$Number[[i]], "0")
# }
print(i)
}
for (i in 1:length(weights$q_name)) {
if (nchar(weights$q_name[i]) != 4) {
weights$q_name[i] <- paste0(weights$q_name[[i]], "0")
}
}
### calculate question weights
weights <- read_csv('data/scheme_q_types.csv')
weights[, 2][weights[, 2] == "Additive"] <- 0.1
weights[, 2][weights[, 2] == "Ambiguous"] <- 0.5
weights[, 2][weights[, 2] == "Contradictory"] <- 1
weights <- weights %>% mutate(weights = as.numeric(Type), q_name = as.character(Number)) %>% select(5, 4)
for (i in 1:length(weights$q_name)) {
if (nchar(weights$q_name[i]) != 4) {
weights$q_name[i] <- paste0(weights$q_name[[i]], "0")
}
}
q_ans_combos <- data.frame(col = colnames(ids_and_features[, -c(1, 2, 297)]))
q_ans_combos$q_name <- substr(q_ans_combos$col, 0, 4)
weights <- weights %>% right_join(q_ans_combos, by = c("q_name"))
weights$weights[is.na(weights$weights)] <- 0
write_csv(weights, "data/scheme_q_weights.csv")
check_cluster <- function(tbl, id, pg_cutoff = 0.9) {
require(dplyr)
require(cluster)
require(fpc)
# weights will be defined outside and are constant regardless of what rows
# are input
# dissimilarity
gower_dist <- tbl %>%
filter(ids == id) %>%
select(-c(task_pybossa_id, contributor_id, ids)) %>%
daisy(metric = c("gower"), weights = weights$weights)
# clustering
aggl_clust <- hclust(gower_dist, method = "complete")
plot(aggl_clust)
# analysis
pg <- c()
for (i in 2:6) {
pg[i - 1] <-
cluster.stats(gower_dist,
clustering = cutree(aggl_clust, k = i))["pearsongamma"][[1]]
if (pg[i - 1] > pg_cutoff) {
return(tbl %>%
filter(ids == id) %>%
select(c(task_pybossa_id, contributor_id)) %>%
mutate(cluster = cutree(aggl_clust, k = i)))
}
}
return(tbl %>%
filter(ids == id) %>%
select(c(task_pybossa_id, contributor_id)) %>%
mutate(cluster = 1))
}
check_cluster(ids_and_features, 1)
check_cluster(ids_and_features, 2)
check_cluster(ids_and_features, 4)
check_cluster(ids_and_features, 5)
dir <- readline("Enter the directory for the `df-canonicalization` folder:")
dir <- readline("Enter the directory for the `df-canonicalization` folder (without quotes):")
dir <- readline("Enter the directory for the `df-canonicalization` folder (without quotes):")
dir <- readline("Enter the directory for the `df-canonicalization` folder (without quotes):")
dir <- readline("Enter the directory for the `df-canonicalization` folder (without quotes):")
setwd(dir)
getwd()
install.packages(c("hash", "XML"))
