col = c("orange", "blue", "green"),
lwd = 3,
cex = 0.8)
plot(class_roc, col = "orange", lwd = 3)
plot(bagged_roc, col = "blue", lwd = 3, add = T)
plot(rf_roc, col = "forest green", lwd = 3, add = T)
abline(0, 1, col = "gray", lty = 2)
title(main = "ROC curves for all classifiers")
legend(0.75, 0.25,
legend = c("CART", "Bagged", "RF"),
col = c("orange", "blue", "forest green"),
lwd = 3,
cex = 0.8)
plot(class_roc, col = "navy", lwd = 3)
plot(bagged_roc, col = "blue", lwd = 3, add = T)
plot(rf_roc, col = "forest green", lwd = 3, add = T)
abline(0, 1, col = "gray", lty = 2)
title(main = "ROC curves for all classifiers")
legend(0.75, 0.25,
legend = c("CART", "Bagged", "RF"),
col = c("navy", "blue", "forest green"),
lwd = 3,
cex = 0.8)
plot(class_roc, col = "orange", lwd = 3)
plot(bagged_roc, col = "navy", lwd = 3, add = T)
plot(rf_roc, col = "forest green", lwd = 3, add = T)
abline(0, 1, col = "gray", lty = 2)
title(main = "ROC curves for all classifiers")
legend(0.75, 0.25,
legend = c("CART", "Bagged", "RF"),
col = c("orange", "navy", "forest green"),
lwd = 3,
cex = 0.8)
mat <- matrix(c(2, -4, -1, 2))
mat <- matrix(c(2, -4, -1, 2), ncol = 2)
View(mat)
svd(mat)
install.packages('devtools')
install.packages('roxygen')
library(devtools)
devtools::install_github("klutometis/roxygen")
setwd('~')
create('datascienceR')
dat <- read.csv("~/Desktop/downloads/dfcrowd1dh_task.csv")
View(dat)
colnames(dat)
dat[, 389]
dat[, 38995]
dat[, 995]
dat[, 840]
dat[, 88]
library(readr)
dfcrowd1dh_task_run <- read_csv("Desktop/downloads/33_dfcrowd1dh_task_run_csv/dfcrowd1dh_task_run.csv")
View(dfcrowd1dh_task_run)
dat <- dfcrowd1dh_task_run
dfcrowd1dh_task_run <- NULL
View(dat)
#####################################
#Some FUNCTIONS for DATE EXTRACTION
#####################################
#This function takes a formatted date as Y-m-d and returns weekday
day_from_date <- function(adate = NULL) {
if (!is.null(adate)) {
day_list <- c("Sunday", "Monday", "Tuesday", "Wednesday", "Thursday", "Friday",
"Saturday")
dow <- try(which(grepl(weekdays(as.Date(adate, "%m-%d-%y")), day_list)), silent=TRUE)
if(inherits(dow, "try-error")) dow <- NA
return(dow)
}
}
# This function takes some text and returns the position on the weekday list
# So return_days("Monday Tuesday Friday")
# will return 2 (for Monday)
return_days <- function(text) {
if(!is.na(text)){
if(is.character(text)) {
day_list <- c("Sunday", "Monday", "Tuesday", "Wednesday", "Thursday", "Friday",
"Saturday")
res <- as.numeric(which(sapply(day_list, grepl, text, ignore.case = TRUE)))
if (length(res) == 0) {
res <- NA
} else {
res<-res[1] # we can add more handling here to deal with multiple weekday mentions
}
}
} else res <- NA
return(res)
}
# Function takes weekday difference between article_date and text_date and
# calculates approximate date.
day_in_text <- function(article_day, aday, pdate) {
if(!is.na(pdate)){
date_diff <- aday - pdate
if (date_diff > 0) {
out <- as.Date(article_day, "%m-%d-%y") - date_diff
} else {
dd <- ifelse(date_diff != 0, date_diff + 7, 0)
out <- as.Date(article_day, "%m-%d-%y") - dd
}
} else{
out <- try(as.Date(article_day, "%m-%d-%y") - 1, silent=TRUE)
if(inherits(out, "try-error")) out <- NA
}
return(out)
}
##############################################
#NOW, to extract DATES from tua text daynames
##############################################
library(lubridate)
#library(dplyr)
library(readr)
all3clean$article_date<-as.character(all3clean$article_date)
library(readr)
dat <- read_csv("Desktop/downloads/33_dfcrowd1dh_task_run_csv/dfcrowd1dh_task_run.csv")
colnames(dat)
dat[1,]
dat[1, 7]
dat[1, 7][1]
dat[1, 7][[1]]
colnames(dat)
library(readr)
huh <- read_csv("Desktop/downloads/33_dfcrowd1dh_task_run_csv/dfcrowd1dh_task_run_info_only.csv")
View(huh[1,])
huh <- NULL
dat_info <- read_csv("Desktop/downloads/dfcrowd1dh_task.csv")
colnames(dat_info[, 1:100])
dat_info[1, 13]
colnames(dat)
dat$task_id
dat_info <- NULL
library(dplyr)
metadata_dat <- read_csv(paste0(dir, "dfcrowd1dh_task.csv"))
paste0(dir, "dfcrowd1dh_task.csv")
# Read in data
dir <- "Desktop/downloads/"
metadata_dat <- read_csv(paste0(dir, "dfcrowd1dh_task.csv"))
### Group task runs by task ID and choose one
dat %>%
group_by(task_id) %>%
slice(1)
### Group task runs by task ID and choose one
training_dat <- dat %>%
group_by(task_id) %>%
slice(1)
### Group task runs by task ID and choose one, merge in publication date
training_dat <- dat %>%
group_by(task_id) %>%
slice(1) %>%
select(c("task_id", "info_article_text", "info_article_id"))
View(training_dat)
day_from_date <- function(adate = NULL) {
if (!is.null(adate)) {
day_list <- c("Sunday", "Monday", "Tuesday", "Wednesday", "Thursday", "Friday",
"Saturday")
dow <- try(which(grepl(weekdays(as.Date(adate, "%m-%d-%y")), day_list)), silent=TRUE)
if(inherits(dow, "try-error")) dow <- NA
return(dow)
}
}
day_from_date("2011-11-02")
day_from_date(as.date("2011-11-02"))
day_from_date(as.Date("2011-11-02"))
return_days <- function(text) {
if(!is.na(text)){
if(is.character(text)) {
day_list <- c("Sunday", "Monday", "Tuesday", "Wednesday", "Thursday", "Friday",
"Saturday")
res <- as.numeric(which(sapply(day_list, grepl, text, ignore.case = TRUE)))
if (length(res) == 0) {
res <- NA
} else {
res<-res[1] # we can add more handling here to deal with multiple weekday mentions
}
}
} else res <- NA
return(res)
}
return_days("Monday Tuesday, Friday")
as.Date("2011-11-02") - 2
library(jsonlite)
read_json("~/Desktop/downloads/dfcrowd1dh-2018-06-21T01.json")
dat <- read_json("~/Desktop/downloads/dfcrowd1dh-2018-06-21T01.json")
results <- dat$results
length(results$text)
results
results <- dat$results[[]]
results <- dat$results
unlist(results)
results[[2]]
results$text
results[1]
dim(results)
length(results)
View(results)
library(readr)
dat <- read_csv("~/Desktop/downloads/dfcrowd1dh-2018-06-21T01.csv")
View(dat)
library(dplyr)
library(reshape2)
View(dat)
molten_dat <- melt(dat, id.vars = c("task_pybossa_id", "contributor_id"))
dat_json <- read_json("~/Desktop/downloads/dfcrowd1dh-2018-06-21T01.json")
results <- dat_json$results
library(jsonlite)
dat_json <- read_json("~/Desktop/downloads/dfcrowd1dh-2018-06-21T01.json")
results <- dat_json$results
View(results)
dat %>% grouped_by(contributor_id, topic_number, question_number)
dat %>% group_by(contributor_id, topic_number, question_number)
dat %>%
group_by(contributor_id, topic_number, question_number) %>%
select(contributor_id, topic_number, question_number, answer_number)
dat %>%
group_by(contributor_id, topic_number, question_number) %>%
select(contributor_id, topic_number, question_number, answer_number) %>%
summarise("count" = n())
dat %>%
group_by(contributor_id, topic_number, question_number) %>%
select(contributor_id, topic_number, question_number, answer_number) %>%
summarise("answer_list" = function(x) {return(list(x))} )
identity
dat %>%
group_by(contributor_id, topic_number, question_number) %>%
select(contributor_id, topic_number, question_number, answer_number) %>%
summarise("answer_list" = identity() )
dat %>%
group_by(contributor_id, topic_number, question_number) %>%
select(contributor_id, topic_number, question_number, answer_number) %>%
summarise("answer_list" = identity())
summarise("answer_list" = identity
dat %>%
group_by(contributor_id, topic_number, question_number) %>%
select(contributor_id, topic_number, question_number, answer_number) %>%
summarise("answer_list" = identity)
dat %>%
group_by(contributor_id, topic_number, question_number)
dat %>%
group_by(contributor_id, topic_number, question_number) %>%
select(contributor_id, topic_number, question_number, answer_number)
dat %>%
group_by(contributor_id, topic_number, question_number) %>%
select(contributor_id, topic_number, question_number, answer_number) %>%
summarise("answer_list" = identity)
dat %>%
group_by(contributor_id, topic_number, question_number) %>%
select(contributor_id, topic_number, question_number, answer_number) %>%
summarise("answer_list" = identity(answer_number))
identity(answer_list)
identity(answer_number)
dat %>%
group_by(contributor_id, topic_number, question_number) %>%
select(contributor_id, topic_number, question_number, answer_number) %>%
summarise("answer_list" = list(answer_number))
dat %>%
group_by(contributor_id, topic_number, question_number) %>%
select(contributor_id, topic_number, question_number, answer_number) %>%
summarise("answer_list" = paste(answer_number))
dat %>%
group_by(contributor_id, topic_number, question_number) %>%
select(contributor_id, topic_number, question_number, answer_number) %>%
summarise(answer_list = paste(answer_number))
dat %>%
group_by(contributor_id, topic_number, question_number) %>%
select(contributor_id, topic_number, question_number, answer_number) %>%
summarise(answer_list = paste(answer_number, collapse = ", "))
dat %>%
group_by(contributor_id, topic_number, question_number, answer_number) %>%
select(contributor_id, topic_number, question_number, answer_number)
dat %>%
group_by(contributor_id, topic_number, question_number, answer_number) %>%
select(contributor_id, topic_number, question_number, answer_number) %>%
summarize(count = n())
dat %>%
filter(task_pybossa_id == 372) %>%
group_by(contributor_id, topic_number, question_number, answer_number) %>%
select(contributor_id, topic_number, question_number, answer_number) %>%
summarize(count = n())
dat %>%
filter(task_pybossa_id == 372) %>%
group_by(contributor_id, topic_number, question_number, answer_number) %>%
select(contributor_id, topic_number, question_number, answer_number) %>%
group_by(contributor_id, topic_number, question_number) %>%
summarize(answer_list = paste(answer_number, collapse = ", "))
dat %>%
filter(task_pybossa_id == 372) %>%
group_by(contributor_id, topic_number, question_number, answer_number) %>%
select(contributor_id, topic_number, question_number, answer_number) %>%
group_by(contributor_id, topic_number, question_number) %>%
summarize(answer_list = list(answer_number))
grouped_dat <- dat %>%
filter(task_pybossa_id == 372) %>%
group_by(contributor_id, topic_number, question_number, answer_number) %>%
select(contributor_id, topic_number, question_number, answer_number) %>%
group_by(contributor_id, topic_number, question_number) %>%
summarize(answer_list = list(answer_number))
View(grouped_dat)
grouped_dat <- dat %>%
filter(task_pybossa_id == 372) %>%
group_by(contributor_id, topic_number, question_number, answer_number) %>%
select(contributor_id, topic_number, question_number, answer_number) %>%
group_by(contributor_id, topic_number, question_number) %>%
summarize(answer_list = unique(answer_number))
grouped_dat <- dat %>%
filter(task_pybossa_id == 372) %>%
group_by(contributor_id, topic_number, question_number, answer_number) %>%
select(contributor_id, topic_number, question_number, answer_number) %>%
group_by(contributor_id, topic_number, question_number) %>%
summarize(answer_list = unique(list(answer_number)))
View(grouped_dat)
grouped_dat <- dat %>%
filter(task_pybossa_id == 372) %>%
group_by(contributor_id, topic_number, question_number, answer_number) %>%
select(contributor_id, topic_number, question_number, answer_number) %>%
group_by(contributor_id, topic_number, question_number) %>%
summarize(answer_list = list(unique(answer_number)))
View(grouped_dat)
grouped_dat <- dat %>%
group_by(task_pybossa_id,
contributor_id,
topic_number,
question_number,
answer_number) %>%
select(task_pybossa_id,
contributor_id,
topic_number,
question_number,
answer_number) %>%
group_by(task_pybossa_id ,
contributor_id,
topic_number,
question_number) %>%
summarize(answer_list = list(unique(answer_number)))
View(grouped_dat)
molten_dat <- melt(dat,
id.vars = c("task_pybossa_id", "contributor_id"),
measure.vars = c("answer_list"))
molten_dat <- melt(grouped_dat,
id.vars = c("task_pybossa_id", "contributor_id"),
measure.vars = c("answer_list"))
molten_dat <- melt(grouped_dat,
id.vars = c("task_pybossa_id", "contributor_id"))
library(readr)
library(dplyr)
###
dir <- "~/df-canonicalization/"
setwd(dir)
###
tua_dat <- read_csv("data/textthresher/DF_Crowd1.0_DataHunt-TUAS.csv")
metadata_dat <- read_csv("data/metadata_table.csv")
metadata_with_tua <- tua_dat %>%
inner_join(metadata_dat,
by = c("article_number" = "info_article_article_number",
"case_number" = "info_highlights_0_case_number")) %>%
select(-c("topic_name")) %>%
select(c(5, 1, 3, 7, 8, 4, 6, 2)) %>%
rename(event_place = info_article_metadata_city,
date_published = info_article_metadata_date_published,
TUA = offsets,
article_text = info_article_text.x)
View(metadata_with_tua)
test_tua <- metadata_with_tua$TUA[1]
test_tua
split_tua <- strsplit(test_tua, '\"')
View(split_tua)
split_tua
seq(1, 10, 2)
seq(2, 10, 2)
seq(2, 11, 2)
without_extra <- split_tua[seq(2, length(split_tua, 2))]
without_extra <- split_tua[seq(2, length(split_tua), 2)]
split_tua <- strsplit(test_tua, '\"')
seq(2, length(split_tua), 2)
length(split_tua)
length(split_tua[1])
length(split_tua[[1]])
without_extra <- split_tua[seq(2, length(split_tua[[1]]), 2)]
View(without_extra)
without_extra <- split_tua[[seq(2, length(split_tua[[1]]), 2)]]
split_tua <- strsplit(test_tua, '\"')
without_extra <- split_tua[[seq(2, length(split_tua[[1]]), 2)]]
View(without_extra)
split_tua[[1]]
unlist(split_tua)
split_tua <- unlist(strsplit(test_tua, '\"'))
without_extra <- split_tua[seq(2, length(split_tua), 2)]
without_extra
without_extra[5]
without_extra[5][1]
gsub("”", "", without_extra[5])
garbage <- c("”", "“")
garbage %in% without_extra[5]
?grep
grep("“", without_extra[5])
grepl(garbage, without_extra[5])
?sapply
?grepl
sapply(garbage, grepl, pattern = split_tua)
?sapply
# check and remove trash characters
garbage_check <- sapply(garbage, grepl, split_tua)
View(garbage_check)
split_tua
View(garbage_check)
split_tua
split_tua[10] <- gsub(garbage[1], "", split_tua[10])
split_tua
# trash characters to remove
garbage <- c("”", "“")
# split input into list of strings
split_tua <- unlist(strsplit(test_tua, '\"'))
# check and remove trash characters
garbage_check <- sapply(garbage, grepl, split_tua)
for (j in 1:ncol(garbage_check)) {
for (i in 1:nrow(garbage_check)) {
if (garbage_check[i, j]) {
split_tua[i] <- gsub(garbage[i], "", split_tua[i])
}
}
}
split_tua
# split input into list of strings
split_tua <- unlist(strsplit(test_tua, '\"'))
# check and remove trash characters
garbage_check <- sapply(garbage, grepl, split_tua)
for (j in 1:ncol(garbage_check)) {
for (i in 1:nrow(garbage_check)) {
if (garbage_check[i, j]) {
split_tua[i] <- gsub(garbage[i], "", split_tua[i])
print(split_tua[i])
}
}
}
garbage_check[i, j]
garbage_check[10, 1]
# split input into list of strings
split_tua <- unlist(strsplit(test_tua, '\"'))
# check and remove trash characters
garbage_check <- sapply(garbage, grepl, split_tua)
for (j in 1:ncol(garbage_check)) {
for (i in 1:nrow(garbage_check)) {
if (garbage_check[i, j]) {
split_tua[i] <- gsub(garbage[j], "", split_tua[i])
print(split_tua[i])
}
}
}
without_extra <- split_tua[seq(2, length(split_tua), 2)]
without_extra
tua_parser <- function(tua_text) {
# trash characters to remove
garbage <- c("”", "“")
# split input into list of strings
split_tua <- unlist(strsplit(test_tua, '\"'))
# check and remove trash characters
garbage_check <- sapply(garbage, grepl, split_tua)
for (j in 1:ncol(garbage_check)) {
for (i in 1:nrow(garbage_check)) {
if (garbage_check[i, j]) {
split_tua[i] <- gsub(garbage[j], "", split_tua[i])
print(split_tua[i])
}}}
without_extra <- split_tua[seq(2, length(split_tua), 2)]
return(without_extra)
}
?apply
sapply(metadata_with_tua$TUA, tua_parser)
###
tua_parser <- function(tua_text) {
# trash characters to remove
garbage <- c("”", "“")
# split input into list of strings
split_tua <- unlist(strsplit(test_tua, '\"'))
# check and remove trash characters
garbage_check <- sapply(garbage, grepl, split_tua)
for (j in 1:ncol(garbage_check)) {
for (i in 1:nrow(garbage_check)) {
if (garbage_check[i, j]) {
split_tua[i] <- gsub(garbage[j], "", split_tua[i])
}}}
without_extra <- split_tua[seq(2, length(split_tua), 2)]
return(without_extra)
}
sapply(metadata_with_tua$TUA, tua_parser)
tua_parser(metadata_with_tua$TUA[1])
###
tua_parser <- function(tua_text) {
# trash characters to remove
garbage <- c("”", "“")
# split input into list of strings
split_tua <- unlist(strsplit(test_tua, '\"'))
# check and remove trash characters
garbage_check <- sapply(garbage, grepl, split_tua)
for (j in 1:ncol(garbage_check)) {
for (i in 1:nrow(garbage_check)) {
if (garbage_check[i, j]) {
split_tua[i] <- gsub(garbage[j], "", split_tua[i])
}}}
without_extra <- split_tua[seq(2, length(split_tua), 2)]
return(list(without_extra))
}
tua_parser(metadata_with_tua$TUA[1])
sapply(metadata_with_tua$TUA, tua_parser)
sapply(metadata_with_tua$TUA, tua_parser)
parsed <- sapply(metadata_with_tua$TUA, tua_parser)
View(parsed)
sapply(metadata_with_tua$TUA, print)
metadata_with_tua$TUA
View(metadata_with_tua)
metadata_with_tua$TUA[1]
metadata_with_tua$TUA[2]
sapply(metadata_with_tua$TUA[1:10], print)
sapply(metadata_with_tua$TUA[1:5], print)
